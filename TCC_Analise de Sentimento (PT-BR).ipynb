{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd08ec9",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67a0bb",
   "metadata": {},
   "source": [
    "### Carregando o dataset a ser utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c57425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matheussilva/anaconda3/envs/p3workshop/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import system\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# imdb_train = pd.read_csv('datasets/csv/imdb_train.csv')\n",
    "# imdb_test = pd.read_csv('datasets/csv/imdb_test.csv')\n",
    "\n",
    "iac_train = pd.read_csv('datasets/csv/iac_convinceme_train.csv', sep= \";\")\n",
    "iac_test = pd.read_csv('datasets/csv/iac_create_debate_test.csv', sep= \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729161f8",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e246dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n",
    "\n",
    "# system(\"mkdir datasets/data_preprocessors\")\n",
    "# system(\"mkdir datasets/vectorized_data\")\n",
    "\n",
    "# Unigram Counts\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "# unigram_vectorizer.fit(imdb_train['text'].values)\n",
    "unigram_vectorizer.fit(iac_train['text'].values)\n",
    "\n",
    "dump(unigram_vectorizer, 'datasets/data_preprocessors/unigram_vectorizer.joblib')\n",
    "\n",
    "# unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')\n",
    "\n",
    "# X_train_unigram = unigram_vectorizer.transform(imdb_train['text'].values)\n",
    "X_train_unigram = unigram_vectorizer.transform(iac_train['text'].values)\n",
    "\n",
    "# Save new model\n",
    "# save_npz('datasets/vectorized_data/X_train_unigram.npz', X_train_unigram)\n",
    "\n",
    "# Load created model\n",
    "X_train_unigram = load_npz('datasets/vectorized_data/X_train_unigram.npz')\n",
    "\n",
    "\n",
    "# Unigram Tf-Idf\n",
    "\n",
    "unigram_tf_idf_transformer = TfidfTransformer()\n",
    "unigram_tf_idf_transformer.fit(X_train_unigram)\n",
    "\n",
    "dump(unigram_tf_idf_transformer, 'datasets/data_preprocessors/unigram_tf_idf_transformer.joblib')\n",
    "\n",
    "# unigram_tf_idf_transformer = load('datasets/data_preprocessors/unigram_tf_idf_transformer.joblib')\n",
    "\n",
    "X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)\n",
    "\n",
    "# Save new model\n",
    "# save_npz('datasets/vectorized_data/X_train_unigram_tf_idf.npz', X_train_unigram_tf_idf)\n",
    "\n",
    "# Load created model\n",
    "X_train_unigram_tf_idf = load_npz('datasets/vectorized_data/X_train_unigram_tf_idf.npz')\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "# Bigram Counts\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "# bigram_vectorizer.fit(imdb_train['text'].values)\n",
    "bigram_vectorizer.fit(iac_train['text'].values)\n",
    "\n",
    "dump(bigram_vectorizer, 'datasets/data_preprocessors/bigram_vectorizer.joblib')\n",
    "\n",
    "# bigram_vectorizer = load('data_preprocessors/bigram_vectorizer.joblib')\n",
    "\n",
    "# X_train_bigram = bigram_vectorizer.transform(imdb_train['text'].values)\n",
    "X_train_bigram = bigram_vectorizer.transform(iac_train['text'].values)\n",
    "\n",
    "# Save new model\n",
    "# save_npz('datasets/vectorized_data/X_train_bigram.npz', X_train_bigram)\n",
    "\n",
    "# Load created model\n",
    "X_train_bigram = load_npz('datasets/vectorized_data/X_train_bigram.npz')\n",
    "\n",
    "\n",
    "# Bigram Tf-Idf\n",
    "\n",
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "\n",
    "dump(bigram_tf_idf_transformer, 'datasets/data_preprocessors/bigram_tf_idf_transformer.joblib')\n",
    "\n",
    "# bigram_tf_idf_transformer = load('datasets/data_preprocessors/bigram_tf_idf_transformer.joblib')\n",
    "\n",
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "\n",
    "# Save new model\n",
    "# save_npz('datasets/vectorized_data/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)\n",
    "\n",
    "# Load created model\n",
    "X_train_bigram_tf_idf = load_npz('datasets/vectorized_data/X_train_bigram_tf_idf.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed808f",
   "metadata": {},
   "source": [
    "## Escolha da melhor solução de Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d936557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Counts\n",
      "Train score: 0.77 ; Validation score: 0.72\n",
      "\n",
      "Unigram Tf-Idf\n",
      "Train score: 0.71 ; Validation score: 0.68\n",
      "\n",
      "Bigram Counts\n",
      "Train score: 0.91 ; Validation score: 0.73\n",
      "\n",
      "Bigram Tf-Idf\n",
      "Train score: 0.73 ; Validation score: 0.67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')\n",
    "\n",
    "# y_train = imdb_train['label'].values\n",
    "y_train = iac_train['sentiment'].values\n",
    "\n",
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c14f5a",
   "metadata": {},
   "source": [
    "## Escolha de melhores parâmetros para a execução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7322d",
   "metadata": {},
   "source": [
    "O melhor método de vetorização de texto para a base IAC, pelos parametros acima, é o Bigram Counts. Com uma Validação de 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6aa242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "#IMDB\n",
    "# X_train = X_train_bigram_tf_idf\n",
    "\n",
    "#IAC\n",
    "X_train = X_train_bigram\n",
    "\n",
    "# Phase 1: loss, learning rate and initial learning rate\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    learning_rate=['optimal', 'invscaling', 'adaptive'],\n",
    "    eta0=uniform(loc=1e-7, scale=1e-2)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50,\n",
    "    dual=False\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2221a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'alpha': 6.0443045621198073e-05, 'penalty': 'elasticnet'}\n",
      "Best score: 0.7369077166254782\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: penalty and alpha\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    penalty=['l1', 'l2', 'elasticnet'],\n",
    "    alpha=uniform(loc=1e-6, scale=1e-4)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed8d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matheussilva/anaconda3/envs/p3workshop/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator SGDClassifier from version 0.23.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# Salvando o melhor classificador\n",
    "\n",
    "# Criar um novo classificador\n",
    "\n",
    "# sgd_classifier = random_search_cv.best_estimator_\n",
    "# dump(random_search_cv.best_estimator_, 'datasets/classifiers/sgd_classifier.joblib')\n",
    "\n",
    "# Carregar classificador\n",
    "sgd_classifier = load('datasets/classifiers/sgd_classifier.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481d6ed",
   "metadata": {},
   "source": [
    "### Base IAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d05e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743890629450299\n"
     ]
    }
   ],
   "source": [
    "X_test = bigram_vectorizer.transform(iac_test['text'].values)\n",
    "y_test = iac_test['sentiment'].values\n",
    "\n",
    "score = sgd_classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e766f",
   "metadata": {},
   "source": [
    "Precisão atingida com a Base IAC - focada em discussões\n",
    "\n",
    "74%\n",
    "\n",
    "Classificações: \n",
    "\n",
    "* positive\n",
    "* neutral\n",
    "* negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11841074",
   "metadata": {},
   "source": [
    "### Classificando a base Reddit e exportar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7127fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'negative', ..., 'positive', 'negative',\n",
       "       'negative'], dtype='<U8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_base = pd.read_csv(\"reddit_post_extraction.csv\")\n",
    "\n",
    "X_reddit = bigram_vectorizer.transform(reddit_base['content'].values)\n",
    "\n",
    "resultado_classificacao = sgd_classifier.predict(X_reddit)\n",
    "resultado_classificacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6700a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output = reddit_base.copy()\n",
    "del csv_output['position']\n",
    "csv_output.insert(6, \"position\",resultado_classificacao, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82bd3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output.to_csv(\"resultado_classificacao.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb48de6",
   "metadata": {},
   "source": [
    "## Base de Dados - IMDB (Comparação)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d89c4b",
   "metadata": {},
   "source": [
    "Precisão encontrada na base de testes é de 90%. Usando como base de treinamento críticas de filmes do site Imdb, e as marcações já presentes na base extraida.\n",
    "\n",
    "O processo de execução com uma base extraida para análise, sem quaisquer marcações é feito abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94f853c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imdb_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sb/lc_nz9bj0pdd7ztvtfbbsrw40000gn/T/ipykernel_20142/2224263468.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram_tf_idf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imdb_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test = bigram_vectorizer.transform(imdb_test['text'].values)\n",
    "X_test = bigram_tf_idf_transformer.transform(X_test)\n",
    "y_test = imdb_test['label'].values\n",
    "\n",
    "score = sgd_classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9e3f5",
   "metadata": {},
   "source": [
    "Para cada uma das linhas no arquivo de testes, há uma célula respectiva no array que a classifica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a628c66",
   "metadata": {},
   "source": [
    "Uma vez obtido o resultado, é possível salvar o resultado obtido em um .csv e usar o notebook CsvToVisu para formatar a saída para o formato aceito pela ferramenta Visu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sgd_classifier.predict(X_test)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb91395",
   "metadata": {},
   "source": [
    "# Métricas de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a64aee",
   "metadata": {},
   "source": [
    "Base de Treino: \"IAC_ConvinceMe_Train.csv\"\n",
    "   * Positive: 52209\n",
    "   * Neutral: 53513\n",
    "   * Negative: 53169\n",
    "   \n",
    "\n",
    "Base de Testes: \"IAC_CreateDebate_test.csv\"\n",
    "   * Postive: 1601\n",
    "   * Neutral: 5608\n",
    "   * Negative: 10346"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06d162",
   "metadata": {},
   "source": [
    "### Matriz de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2626b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab201c2",
   "metadata": {},
   "source": [
    "* <h5> Usando base de teste create_debate </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Base de teste: iac_create_debate_test.csv\n",
    "\n",
    "create_debate = pd.read_csv('datasets/csv/iac_create_debate_test.csv', sep= \";\")\n",
    "\n",
    "X = bigram_vectorizer.transform(create_debate['text'].values)\n",
    "Y = create_debate['sentiment'].values\n",
    "\n",
    "figura = plt.figure(figsize=(15,15))\n",
    "plot_confusion_matrix(sgd_classifier, X, Y, cmap=plt.cm.Greens, display_labels=['oposição', 'neutro', 'apoio'])\n",
    "plt.savefig('Matriz_Confusao_CreateDebate.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444b84c",
   "metadata": {},
   "source": [
    "   * <h5> Usando base de teste com sample aleatório (12% da base o ConvinceMe)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f72729",
   "metadata": {},
   "outputs": [],
   "source": [
    "convince_me = pd.read_csv('datasets/csv/iac_convinceme_original_full.csv', sep= \";\").sample(frac=.12)\n",
    "\n",
    "X = bigram_vectorizer.transform(convince_me['text'].values)\n",
    "Y = convince_me['sentiment'].values\n",
    "\n",
    "figura = plt.figure(figsize=(15,15))\n",
    "plot_confusion_matrix(sgd_classifier, X, Y, cmap=plt.cm.Greens, display_labels=['oposição', 'neutro', 'apoio'])\n",
    "plt.savefig('Matriz_Confusao_ConvinceMe10Percent.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492982f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
